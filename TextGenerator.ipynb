{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqLuyzqlCb2w",
        "outputId": "766b7e4d-cc17-416f-85c5-fbfb29692701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# import dependencies\n",
        "import numpy\n",
        "import sys\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MsDhT5Y8EMWC"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "file = open (\"/content/meta_text.txt\").read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HSt9S5jAGtk-"
      },
      "outputs": [],
      "source": [
        "# tokenization\n",
        "# standardization\n",
        "def tokenize_words(input_text):\n",
        "    input_text = input_text.lower()\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(input_text)\n",
        "    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
        "    return \" \".join(filtered_tokens)\n",
        "processed_inputs = tokenize_words(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oupWLplQo8rW"
      },
      "outputs": [],
      "source": [
        "# chars to numbers\n",
        "chars = sorted(list(set(processed_inputs)))\n",
        "char_to_num = dict((c, i) for i, c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zl9lqwmko9pG",
        "outputId": "6d962f08-8545-4f0d-e8d1-26b148bb38f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters: 67092\n",
            "Total vocab: 28\n"
          ]
        }
      ],
      "source": [
        "# check if words to chars or chars to num (?!) has worked?\n",
        "input_len = len(processed_inputs)\n",
        "vocab_len = len(chars)\n",
        "print(\"Total number of characters:\", input_len)\n",
        "print(\"Total vocab:\", vocab_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KFxxTyr2pSVg"
      },
      "outputs": [],
      "source": [
        "# seg length\n",
        "seq_length = 100\n",
        "x_data = []\n",
        "y_data = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Zlc7r1TpwD-",
        "outputId": "402f6137-454b-4f0a-c85a-b20e0d155d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Patterns: 66992\n"
          ]
        }
      ],
      "source": [
        "#loop through the seguence\n",
        "for i in range(0, input_len - seq_length, 1):\n",
        "    in_seg = processed_inputs[i:i + seq_length]\n",
        "    out_seg = processed_inputs[i + seq_length]\n",
        "    x_data.append([char_to_num[char] for char in in_seg])\n",
        "    y_data.append(char_to_num[out_seg])\n",
        "n_patterns = len(x_data)\n",
        "print(\"Total Patterns:\", n_patterns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "r1no_LYNrKwh"
      },
      "outputs": [],
      "source": [
        "# convert input sequence to np array that our network can use\n",
        "x = numpy.reshape(x_data, (n_patterns,seq_length, 1))\n",
        "x = x / float(vocab_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "w8O8jNKVtXX7"
      },
      "outputs": [],
      "source": [
        "# one-hot encoding our label data\n",
        "y = to_categorical(y_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YaZgk4YZuDvF"
      },
      "outputs": [],
      "source": [
        "#creating the model\n",
        "# creating a sequential model\n",
        "# dropout is used to prevent overfitting\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(x.shape[1], x.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4flC9bGRujiY"
      },
      "outputs": [],
      "source": [
        "#compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cHXj9xPRu7t7"
      },
      "outputs": [],
      "source": [
        "#saving weights\n",
        "filepath = \"model_weights_saved.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "desired_callbacks = [checkpoint]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBl9algrvXJf",
        "outputId": "523a7ed2-a89d-4f68-ca99-3182a4891b33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "262/262 [==============================] - ETA: 0s - loss: 2.9376\n",
            "Epoch 1: loss improved from inf to 2.93756, saving model to model_weights_saved.hdf5\n",
            "262/262 [==============================] - 1072s 4s/step - loss: 2.9376\n",
            "Epoch 2/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "262/262 [==============================] - ETA: 0s - loss: 2.8975\n",
            "Epoch 2: loss improved from 2.93756 to 2.89753, saving model to model_weights_saved.hdf5\n",
            "262/262 [==============================] - 1028s 4s/step - loss: 2.8975\n",
            "Epoch 3/4\n",
            "262/262 [==============================] - ETA: 0s - loss: 2.7011\n",
            "Epoch 3: loss improved from 2.89753 to 2.70110, saving model to model_weights_saved.hdf5\n",
            "262/262 [==============================] - 1022s 4s/step - loss: 2.7011\n",
            "Epoch 4/4\n",
            "262/262 [==============================] - ETA: 0s - loss: 2.5056\n",
            "Epoch 4: loss improved from 2.70110 to 2.50557, saving model to model_weights_saved.hdf5\n",
            "262/262 [==============================] - 1031s 4s/step - loss: 2.5056\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7891381ed6f0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#fit model and let it train\n",
        "model.fit(x , y, epochs=4, batch_size=256, callbacks=desired_callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fAz_H8FV3uLK"
      },
      "outputs": [],
      "source": [
        "#recompile model with the saved weights\n",
        "filename = \"model_weights_saved.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fCYFHzlc3yKs"
      },
      "outputs": [],
      "source": [
        "#output of the model back into characters\n",
        "num_to_char = dict((i, c) for i, c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uL7JR32R4BPr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca4b5394-f4df-4247-88ca-369255023dcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Seed:\n",
            "ed opened eyes wide whistled waste time yank open bedroom doors shout loudly darkness bedrooms come \n"
          ]
        }
      ],
      "source": [
        "# random seed to help generate\n",
        "import numpy as np\n",
        "start = numpy.random.randint(0, len(x_data) - 1)\n",
        "pattern = x_data[start]\n",
        "print(\"Random Seed:\")\n",
        "print(\"\".join([num_to_char[value] for value in pattern]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YoYft7s84gKD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9e25e4d-6b2d-49a1-cd1c-af9d2aeda9f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e sooe sooe sooe sooe sooe sooe sooe sooe sooe sooe sooe sooe sooe sooe sooe sooe sooe sooe sooe soo"
          ]
        }
      ],
      "source": [
        "# generate the text\n",
        "for i in range(100):\n",
        "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(vocab_len)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = np.argmax(prediction)\n",
        "    result = num_to_char[index]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}